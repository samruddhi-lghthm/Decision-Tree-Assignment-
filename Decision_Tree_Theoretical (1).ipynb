{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work?"
      ],
      "metadata": {
        "id": "ICiNX5_v5nPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-like model of decisions, where each internal node represents a decision (based on an attribute), each branch represents an outcome of the decision, and each leaf node represents a class label or continuous value (for regression)."
      ],
      "metadata": {
        "id": "N1sp1wRY5v1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees?"
      ],
      "metadata": {
        "id": "2lniU_Gs5-6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Impurity measures help determine the quality of a split at each node in a Decision Tree. They quantify how mixed or homogeneous the data is within a node. The goal is to minimize impurity, making the data in each split as pure (homogeneous) as possible."
      ],
      "metadata": {
        "id": "KEzO8S8F6C-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "KbdfUzjh6L5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it were labeled randomly according to the class distribution.\n"
      ],
      "metadata": {
        "id": "VqgsRTNb6RAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "7LvHpYjN62n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Entropy measures the randomness (or impurity) in a dataset. It is used in decision trees to evaluate how well a feature splits the data.\n",
        "\n",
        "Formula:\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )"
      ],
      "metadata": {
        "id": "P_u49xMt7AY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "IiWeKCvi7fiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Information Gain (IG) measures the reduction in entropy after splitting a dataset on a feature, helping decision trees select the most informative attribute for making splits at each node."
      ],
      "metadata": {
        "id": "x2CmtVSr7sWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "RG1xsg9V8Ek5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Gini Impurity measures the probability of incorrect classification if a random instance is labeled based on class distribution, while Entropy quantifies the disorder in a node, with entropy involving logarithms and being slightly more computationally intensive."
      ],
      "metadata": {
        "id": "cg4CWXF_8a4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees?"
      ],
      "metadata": {
        "id": "6SSGgedq8n0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Decision Trees recursively split data based on feature values to minimize impurity—measured using **Gini Impurity** (\\( 1 - \\sum p_i^2 \\)) or **Entropy** (\\( -\\sum p_i \\log_2 p_i \\))—until reaching pure or nearly pure leaf nodes, optimizing splits using **Information Gain** (\\( IG = Entropy_{parent} - \\sum \\frac{|S_j|}{|S|} Entropy_{S_j} \\))."
      ],
      "metadata": {
        "id": "d88O_6Nm8wxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "O-N4orT089r5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : Pre-pruning in Decision Trees is the process of stopping tree growth early by setting constraints like maximum depth, minimum samples per split, or minimum information gain to prevent overfitting and improve generalization."
      ],
      "metadata": {
        "id": "NudFP9ur9DnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Post-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "Ftwie0kG9Wpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : **Post-pruning** in Decision Trees involves growing the full tree first and then trimming back branches that provide little to no improvement in predictive performance, typically using techniques like cost-complexity pruning or validation on a separate dataset to prevent overfitting."
      ],
      "metadata": {
        "id": "WyUiREd_9cTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning?"
      ],
      "metadata": {
        "id": "jjr--nA_9_yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : **Pre-pruning** stops the growth of a decision tree early by setting constraints (e.g., max depth, min samples per split) to prevent overfitting, while **post-pruning** first grows the full tree and then trims back less significant branches using validation or cost-complexity pruning for better generalization."
      ],
      "metadata": {
        "id": "l610_9c3-L0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "TvH9nqrw-bL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A **Decision Tree Regressor** is a decision tree algorithm used for regression tasks, where it splits data based on feature values and predicts continuous target values by averaging or taking the majority value of data points in each leaf node."
      ],
      "metadata": {
        "id": "q5MevSZQ-ueP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  What are the advantages and disadvantages of Decision Trees?"
      ],
      "metadata": {
        "id": "HXVxrDce_fvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans :  **Advantages** of Decision Trees include interpretability, handling of both numerical and categorical data, and no need for feature scaling, while **disadvantages** include susceptibility to overfitting, sensitivity to small data variations, and bias toward features with more categories."
      ],
      "metadata": {
        "id": "lsBxkQQM_plV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  How does a Decision Tree handle missing values?"
      ],
      "metadata": {
        "id": "KS9ebzyOAAox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A **Decision Tree** handles missing values by using techniques such as ignoring missing data during splitting, imputing missing values with the most frequent or mean value, or assigning probabilities to different paths based on existing data distributions."
      ],
      "metadata": {
        "id": "8NaLEBLfAGD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  How does a Decision Tree handle categorical features?"
      ],
      "metadata": {
        "id": "Mro-7aWCAYmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans : A **Decision Tree** handles categorical features by computing impurity measures (e.g., Gini Impurity or Entropy) for each category, selecting the best split based on Information Gain, and either creating separate branches for each category or encoding them numerically for efficient processing."
      ],
      "metadata": {
        "id": "doPAvwxzAk5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "en_y34pyAqoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r5HquRbvA2GB"
      }
    }
  ]
}